{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q imageio\n",
    "# !pip install -q git+https://github.com/tensorflow/docs\n",
    "# !pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 구동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cache 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import imageio\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "# KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "#     (0, 1): (255, 0, 255),\n",
    "#     (0, 2): (0, 255, 255),\n",
    "#     (1, 3): (255, 0, 255),\n",
    "#     (2, 4): (0, 255, 255),\n",
    "#     (0, 5): (255, 0, 255),\n",
    "#     (0, 6): (0, 255, 255),\n",
    "#     (5, 7): (255, 0, 255),\n",
    "#     (7, 9): (255, 0, 255),\n",
    "#     (6, 8): (0, 255, 255),\n",
    "#     (8, 10): (0, 255, 255),\n",
    "#     (5, 6): (0, 255, 0),\n",
    "#     (5, 11): (255, 0, 255),\n",
    "#     (6, 12): (0, 255, 255),\n",
    "#     (11, 12): (0, 255, 0),\n",
    "#     (11, 13): (255, 0, 255),\n",
    "#     (13, 15): (255, 0, 255),\n",
    "#     (12, 14): (0, 255, 255),\n",
    "#     (14, 16): (0, 255, 255)\n",
    "# }\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)  \n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n",
    "\n",
    "# def draw_keypoints(frame, keypoints, confidence_threshold=0.3):\n",
    "#   \"\"\"OpenCV를 사용하여 keypoints와 스켈레톤 그리기\"\"\"\n",
    "#   y, x, c = frame.shape\n",
    "#   shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
    "\n",
    "#   # 각 keypoint 그리기\n",
    "#   for kp in shaped:\n",
    "#     ky, kx, kp_conf = kp\n",
    "#     if kp_conf > confidence_threshold:\n",
    "#       cv2.circle(frame, (int(kx), int(ky)), 4, (0, 255, 0), -1)\n",
    "\n",
    "#   # keypoint들 연결 선 그리기\n",
    "#   for edge, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "#     p1, p2 = edge\n",
    "#     y1, x1, c1 = shaped[p1]\n",
    "#     y2, x2, c2 = shaped[p2]\n",
    "#     if c1 > confidence_threshold and c2 > confidence_threshold:\n",
    "#       cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "\n",
    "\n",
    "\n",
    "def to_gif(images, duration):\n",
    "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "  imageio.mimsave('./animation.gif', images, duration=duration)\n",
    "  return embed.embed_file('./animation.gif')\n",
    "\n",
    "def progress(value, max=100):\n",
    "  return HTML(\"\"\"\n",
    "      \n",
    "          {value}\n",
    "      \n",
    "  \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
    "\n",
    "if \"tflite\" in model_name:\n",
    "  if \"movenet_lightning_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  elif \"movenet_lightning_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  # Initialize the TFLite interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores\n",
    "\n",
    "else:\n",
    "  if \"movenet_lightning\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Cropping Algorithm\n",
    "\n",
    "# Confidence score to determine whether a keypoint prediction is reliable.\n",
    "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "\n",
    "def init_crop_region(image_height, image_width):\n",
    "  \"\"\"Defines the default crop region.\n",
    "\n",
    "  The function provides the initial crop region (pads the full image from both\n",
    "  sides to make it a square image) when the algorithm cannot reliably determine\n",
    "  the crop region from the previous frame.\n",
    "  \"\"\"\n",
    "  if image_width > image_height:\n",
    "    box_height = image_width / image_height\n",
    "    box_width = 1.0\n",
    "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "    x_min = 0.0\n",
    "  else:\n",
    "    box_height = 1.0\n",
    "    box_width = image_height / image_width\n",
    "    y_min = 0.0\n",
    "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "\n",
    "  return {\n",
    "    'y_min': y_min,\n",
    "    'x_min': x_min,\n",
    "    'y_max': y_min + box_height,\n",
    "    'x_max': x_min + box_width,\n",
    "    'height': box_height,\n",
    "    'width': box_width\n",
    "  }\n",
    "\n",
    "def torso_visible(keypoints):\n",
    "  \"\"\"Checks whether there are enough torso keypoints.\n",
    "\n",
    "  This function checks whether the model is confident at predicting one of the\n",
    "  shoulders/hips which is required to determine a good crop region.\n",
    "  \"\"\"\n",
    "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE) and\n",
    "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE or\n",
    "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
    "           MIN_CROP_KEYPOINT_SCORE))\n",
    "\n",
    "def determine_torso_and_body_range(\n",
    "    keypoints, target_keypoints, center_y, center_x):\n",
    "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
    "\n",
    "  The function returns the maximum distances from the two sets of keypoints:\n",
    "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
    "  used to determine the crop size. See determineCropRegion for more detail.\n",
    "  \"\"\"\n",
    "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "  max_torso_yrange = 0.0\n",
    "  max_torso_xrange = 0.0\n",
    "  for joint in torso_joints:\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "    if dist_y > max_torso_yrange:\n",
    "      max_torso_yrange = dist_y\n",
    "    if dist_x > max_torso_xrange:\n",
    "      max_torso_xrange = dist_x\n",
    "\n",
    "  max_body_yrange = 0.0\n",
    "  max_body_xrange = 0.0\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
    "      continue\n",
    "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
    "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
    "    if dist_y > max_body_yrange:\n",
    "      max_body_yrange = dist_y\n",
    "\n",
    "    if dist_x > max_body_xrange:\n",
    "      max_body_xrange = dist_x\n",
    "\n",
    "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
    "\n",
    "def determine_crop_region(\n",
    "      keypoints, image_height,\n",
    "      image_width):\n",
    "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
    "\n",
    "  The algorithm uses the detected joints from the previous frame to estimate\n",
    "  the square region that encloses the full body of the target person and\n",
    "  centers at the midpoint of two hip joints. The crop size is determined by\n",
    "  the distances between each joints and the center point.\n",
    "  When the model is not confident with the four torso joint predictions, the\n",
    "  function returns a default crop which is the full image padded to square.\n",
    "  \"\"\"\n",
    "  target_keypoints = {}\n",
    "  for joint in KEYPOINT_DICT.keys():\n",
    "    target_keypoints[joint] = [\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
    "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
    "    ]\n",
    "\n",
    "  if torso_visible(keypoints):\n",
    "    center_y = (target_keypoints['left_hip'][0] +\n",
    "                target_keypoints['right_hip'][0]) / 2;\n",
    "    center_x = (target_keypoints['left_hip'][1] +\n",
    "                target_keypoints['right_hip'][1]) / 2;\n",
    "\n",
    "    (max_torso_yrange, max_torso_xrange,\n",
    "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
    "          keypoints, target_keypoints, center_y, center_x)\n",
    "\n",
    "    crop_length_half = np.amax(\n",
    "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
    "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
    "\n",
    "    tmp = np.array(\n",
    "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
    "    crop_length_half = np.amin(\n",
    "        [crop_length_half, np.amax(tmp)]);\n",
    "\n",
    "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
    "\n",
    "    if crop_length_half > max(image_width, image_height) / 2:\n",
    "      return init_crop_region(image_height, image_width)\n",
    "    else:\n",
    "      crop_length = crop_length_half * 2;\n",
    "      return {\n",
    "        'y_min': crop_corner[0] / image_height,\n",
    "        'x_min': crop_corner[1] / image_width,\n",
    "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
    "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
    "        'height': (crop_corner[0] + crop_length) / image_height -\n",
    "            crop_corner[0] / image_height,\n",
    "        'width': (crop_corner[1] + crop_length) / image_width -\n",
    "            crop_corner[1] / image_width\n",
    "      }\n",
    "  else:\n",
    "    return init_crop_region(image_height, image_width)\n",
    "\n",
    "def crop_and_resize(image, crop_region, crop_size):\n",
    "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
    "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
    "          crop_region['y_max'], crop_region['x_max']]]\n",
    "  output_image = tf.image.crop_and_resize(\n",
    "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
    "  return output_image\n",
    "\n",
    "def run_inference(movenet, image, crop_region, crop_size):\n",
    "  \"\"\"Runs model inferece on the cropped region.\n",
    "\n",
    "  The function runs the model inference on the cropped region and updates the\n",
    "  model output to the original image coordinate system.\n",
    "  \"\"\"\n",
    "  image_height, image_width, _ = image.shape\n",
    "  input_image = crop_and_resize(\n",
    "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
    "  # Run model inference.\n",
    "  keypoints_with_scores = movenet(input_image)\n",
    "  # Update the coordinates.\n",
    "  for idx in range(17):\n",
    "    keypoints_with_scores[0, 0, idx, 0] = (\n",
    "        crop_region['y_min'] * image_height +\n",
    "        crop_region['height'] * image_height *\n",
    "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
    "    keypoints_with_scores[0, 0, idx, 1] = (\n",
    "        crop_region['x_min'] * image_width +\n",
    "        crop_region['width'] * image_width *\n",
    "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
    "  return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -q -O dance.gif https://github.com/tensorflow/tfjs-models/raw/master/pose-detection/assets/dance_input.gif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(keypoints_with_scores):\n",
    "    \"\"\"관절 좌표를 기반으로 특징을 추출\"\"\"\n",
    "    keypoints = keypoints_with_scores[0, 0, :, :2]  # 각 프레임의 관절 좌표만 추출\n",
    "    keypoints = keypoints.flatten()\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def prepare_dataset(videos):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for video, label in videos:\n",
    "#         cap = cv2.VideoCapture(video)\n",
    "#         while cap.isOpened():\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             # 이미지를 movenet 입력 사이즈에 맞게 조정\n",
    "#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             resized_frame = cv2.resize(frame_rgb, (input_size, input_size))\n",
    "#             input_image = tf.image.resize_with_pad(np.expand_dims(resized_frame, axis=0), input_size, input_size)\n",
    "#             input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "\n",
    "#             # Movenet으로 관절 추출\n",
    "#             keypoints_with_scores = movenet(input_image)\n",
    "#             features = extract_features(keypoints_with_scores)\n",
    "#             X.append(features)\n",
    "#             y.append(label) # 0: ADL, 1: Fall\n",
    "#         cap.release()\n",
    "#     return np.array(X), np.array(y)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_dataset 함수를 이미지 파일도 처리할 수 있도록 수정\n",
    "def prepare_dataset(videos_or_images):\n",
    "    X = []\n",
    "    y = []\n",
    "    for file_path, label in videos_or_images:\n",
    "        if file_path.endswith('.mp4'):\n",
    "            # 비디오 파일 처리\n",
    "            cap = cv2.VideoCapture(file_path)\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # 프레임 크기 축소\n",
    "                frame = cv2.resize(frame, (640, 480))\n",
    "                \n",
    "                # Movenet을 통해 keypoints 추출\n",
    "                input_image = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), input_size, input_size)\n",
    "                input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "                keypoints_with_scores = movenet(input_image)\n",
    "                \n",
    "                # 특징 추출\n",
    "                features = extract_features(keypoints_with_scores)\n",
    "                X.append(features)\n",
    "                y.append(label)\n",
    "            cap.release()\n",
    "        elif file_path.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            # 이미지 파일 처리\n",
    "            image = cv2.imread(file_path)\n",
    "            image = cv2.resize(image, (640, 480))\n",
    "            \n",
    "            # Movenet을 통해 keypoints 추출\n",
    "            input_image = tf.image.resize_with_pad(np.expand_dims(image, axis=0), input_size, input_size)\n",
    "            input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "            keypoints_with_scores = movenet(input_image)\n",
    "            \n",
    "            # 특징 추출\n",
    "            features = extract_features(keypoints_with_scores)\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 테스트 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "videos_or_images = [(\"./data/pose_est/ADL/01.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/02.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/03.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/04.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/05.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/06.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/07.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/08.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/09.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/10.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/11.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/12.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/13.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/14.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/15.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/16.mp4\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen001.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen002.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen003.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen004.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen005.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen006.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen007.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen008.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen009.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen010.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen011.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen012.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen013.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen014.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen015.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen016.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen017.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen018.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen019.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen020.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen021.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen022.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen023.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen024.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen025.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen026.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen027.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen028.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen029.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen030.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen040.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen041.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen042.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen043.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen044.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen045.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen046.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen047.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen048.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen049.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen050.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen051.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen052.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen053.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen054.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen055.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen056.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen057.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen058.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen059.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen060.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen061.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen062.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen063.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen064.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen065.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen066.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen067.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen068.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen069.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen070.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen071.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen072.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen073.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen074.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen075.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen076.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen077.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen078.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen079.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen080.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen081.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen082.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen083.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen084.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen085.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen086.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen087.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen088.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen089.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen090.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen091.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen092.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen093.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen094.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen095.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen096.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen097.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen098.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen099.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen100.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen101.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen102.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen103.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen104.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen105.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen106.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen107.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen108.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen109.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen110.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen111.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen112.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen113.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen114.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen115.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen116.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen117.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen118.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen119.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen120.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen121.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen122.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen123.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen124.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen125.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen126.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen127.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen128.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen129.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen130.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen131.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen132.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen133.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen134.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen135.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen136.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen137.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen138.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen139.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen140.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen141.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen142.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen143.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen144.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen145.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen146.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen147.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen148.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen149.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen150.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen151.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen152.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen153.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen154.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen155.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen156.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen157.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen158.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen159.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen160.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen161.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen162.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen163.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen164.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen165.jpg\", 0),\n",
    "          (\"./data/pose_est/ADL/not fallen166.jpg\", 0),\n",
    "\n",
    "           (\"./data/pose_est/Fall/01.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/02.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/03.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/04.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/05.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/06.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/07.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/08.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/09.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/10.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/11.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/12.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/13.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/14.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/15.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/16.mp4\", 1),\n",
    "           (\"./data/pose_est/Fall/fall001.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall002.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall003.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall004.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall005.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall006.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall007.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall008.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall009.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall010.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall011.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall012.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall013.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall014.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall015.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall016.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall017.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall018.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall019.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall020.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall021.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall022.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall023.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall024.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall025.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall026.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall027.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall028.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall029.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall030.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall031.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall032.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall033.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall034.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall035.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall036.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall037.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall038.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall039.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall040.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall041.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall042.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall043.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall044.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall045.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall046.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall047.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall048.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall049.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall050.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall051.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall052.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall053.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall054.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall055.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall056.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall057.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall058.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall059.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall060.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall061.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall062.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall063.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall064.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall065.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall066.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall067.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall068.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall069.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall070.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall071.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall072.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall073.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall074.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall075.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall076.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall077.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall078.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall079.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall080.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall081.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall082.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall083.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall084.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall085.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall086.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall087.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall088.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall089.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall090.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall091.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall092.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall093.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall094.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall095.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall096.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall097.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall098.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall099.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall100.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall101.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall102.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall103.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall104.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall105.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall106.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall107.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall108.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall109.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall110.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall111.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall112.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall113.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall114.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall115.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall116.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall117.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall118.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall119.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall120.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall121.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall122.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall123.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall124.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall125.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall126.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall127.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall128.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall129.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall130.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall131.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall132.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall133.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall134.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall135.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall136.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall137.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall138.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall139.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall140.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall141.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall142.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall143.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall144.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall145.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall146.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall147.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall148.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall149.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall150.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall151.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall152.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall153.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall154.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall155.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall156.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall157.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall158.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall159.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall160.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall161.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall162.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall163.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall164.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall165.jpg\", 1),\n",
    "           (\"./data/pose_est/Fall/fall166.jpg\", 1)\n",
    "           ]\n",
    "X, y = prepare_dataset(videos_or_images)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=200)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8436507936507937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 녹화 비디오 탐지 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_fall_detection(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        image_height, image_width, _ = frame.shape\n",
    "        crop_region = init_crop_region(image_height, image_width)\n",
    "        keypoints_with_scores = run_inference(movenet, frame, crop_region, crop_size=(input_size, input_size))\n",
    "        # 특징 추출\n",
    "        features = extract_features(keypoints_with_scores)\n",
    "\n",
    "        # 추락 여부 추측\n",
    "        is_fall = model.predict([features])[0]\n",
    "        if is_fall:\n",
    "            # print(\"추락 감지!\")\n",
    "            cv2.putText(frame, \"Fall Detected!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # keypoints 시각화\n",
    "        frame_with_keypoints = draw_prediction_on_image(frame, keypoints_with_scores)\n",
    "\n",
    "\n",
    "        # 결과 프레임 보여주기\n",
    "        cv2.imshow(\"Fall Detection\", frame)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "real_time_fall_detection(\"./data/pose_est/Fall/12.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실시간 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_fall_detection(video_source=0):\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        image_height, image_width, _ = frame.shape\n",
    "        crop_region = init_crop_region(image_height, image_width)\n",
    "        keypoints_with_scores = run_inference(movenet, frame, crop_region, crop_size=(input_size, input_size))\n",
    "        # 특징 추출\n",
    "        features = extract_features(keypoints_with_scores)\n",
    "\n",
    "        # 추락 여부 추측\n",
    "        is_fall = model.predict([features])[0]\n",
    "        if is_fall:\n",
    "            # print(\"추락 감지!\")\n",
    "            cv2.putText(frame, \"Fall Detected!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # keypoints 시각화\n",
    "        frame_with_keypoints = draw_prediction_on_image(frame, keypoints_with_scores)\n",
    "\n",
    "\n",
    "        # 결과 프레임 보여주기\n",
    "        cv2.imshow(\"Fall Detection\", frame_with_keypoints)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "real_time_fall_detection(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n"
     ]
    }
   ],
   "source": [
    "# real_time_fall_detection 수정: OpenCV 시각화 추가 및 속도 최적화\n",
    "def real_time_fall_detection(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 프레임 크기 축소\n",
    "        frame = cv2.resize(frame, (640, 480))  # 프레임 크기를 640x480으로 축소\n",
    "        \n",
    "        # Movenet을 통해 keypoints 추론\n",
    "        input_image = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), input_size, input_size)\n",
    "        input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "        keypoints_with_scores = movenet(input_image)\n",
    "        # 특징 추출\n",
    "        features = extract_features(keypoints_with_scores)\n",
    "        # 추락 여부 추측\n",
    "        is_fall = model.predict([features])[0]\n",
    "        if is_fall:\n",
    "            print(\"추락 감지!\")\n",
    "            cv2.putText(frame, \"Fall Detected!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # 특징 추출 및 추락 감지 (여기서는 생략, 대신 시각화)\n",
    "        draw_keypoints(frame, keypoints_with_scores)\n",
    "\n",
    "        # 결과 프레임 보여주기\n",
    "        cv2.imshow(\"Fall Detection\", frame)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 예시 실행\n",
    "real_time_fall_detection(\"./data/pose_est/Fall/15.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n",
      "추락 감지!\n"
     ]
    }
   ],
   "source": [
    "# real_time_fall_detection 수정: OpenCV 시각화 추가 및 속도 최적화\n",
    "def real_time_fall_detection(video_source=0):\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 프레임 크기 축소\n",
    "        frame = cv2.resize(frame, (640, 480))  # 프레임 크기를 640x480으로 축소\n",
    "        \n",
    "        # Movenet을 통해 keypoints 추론\n",
    "        input_image = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), input_size, input_size)\n",
    "        input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "        keypoints_with_scores = movenet(input_image)\n",
    "        # 특징 추출\n",
    "        features = extract_features(keypoints_with_scores)\n",
    "        # 추락 여부 추측\n",
    "        is_fall = model.predict([features])[0]\n",
    "        if is_fall:\n",
    "            print(\"추락 감지!\")\n",
    "            cv2.putText(frame, \"Fall Detected!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # 특징 추출 및 추락 감지 (여기서는 생략, 대신 시각화)\n",
    "        draw_keypoints(frame, keypoints_with_scores)\n",
    "\n",
    "        # 결과 프레임 보여주기\n",
    "        cv2.imshow(\"Fall Detection\", frame)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 예시 실행\n",
    "real_time_fall_detection(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load the input image.\n",
    "# image_path = 'dance.gif'\n",
    "# image = tf.io.read_file(image_path)\n",
    "# image = tf.image.decode_gif(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the input image.\n",
    "# num_frames, image_height, image_width, _ = image.shape\n",
    "# crop_region = init_crop_region(image_height, image_width)\n",
    "\n",
    "# output_images = []\n",
    "# bar = display(progress(0, num_frames-1), display_id=True)\n",
    "# for frame_idx in range(num_frames):\n",
    "#   keypoints_with_scores = run_inference(\n",
    "#       movenet, image[frame_idx, :, :, :], crop_region,\n",
    "#       crop_size=[input_size, input_size])\n",
    "#   output_images.append(draw_prediction_on_image(\n",
    "#       image[frame_idx, :, :, :].numpy().astype(np.int32),\n",
    "#       keypoints_with_scores, crop_region=None,\n",
    "#       close_figure=True, output_image_height=300))\n",
    "#   crop_region = determine_crop_region(\n",
    "#       keypoints_with_scores, image_height, image_width)\n",
    "#   bar.update(progress(frame_idx, num_frames-1))\n",
    "\n",
    "# # Prepare gif visualization.\n",
    "# output = np.stack(output_images, axis=0)\n",
    "# to_gif(output, duration=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

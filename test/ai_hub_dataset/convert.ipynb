{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-HUB data label 을 labelme format 으로 변환\n",
    "\n",
    "* 의상 정보를 훈련하기 위한 datast 으로 다음의 데이터를 사용하고자 함. [의류 통합 데이터(착용 이미지, 치수 및 원단 정보)](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=data&dataSetSn=71501)\n",
    "* 해당 data 를 YOLOv8 segment 훈련을 진행하기 위해서는 lebel 데이터를 변환해야 한다. \n",
    "* AI-HUB label json -> labelme json 으로 변환 후 `labelme2yolo --json_dir <./json_dir/> --output_format polygon` 을 사용하여 최종 YOLO lebel 인 txt 파일료 번환하여 사용하면 된다.\n",
    "* 다만, 훈련 결과가 좋지 않아 다른 dataset 을 사용하여 학습하였음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_all_paths(directory):\n",
    "    file_paths = []\n",
    "\n",
    "    # os.walk()  디렉토리 경로, 하위 디렉토리 목록, 파일 목록 반환\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292621\n"
     ]
    }
   ],
   "source": [
    "origin_json_file_list = get_all_paths(\"./dataset/origin_labels\")\n",
    "print(len(origin_json_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_labelme(file_path):\n",
    "    with open(file_path, \"r\") as read_json:\n",
    "        data = json.load(read_json)\n",
    "\n",
    "        try:\n",
    "            annotation_points = data[\"annotation\"][0][\"annotation_point\"]\n",
    "        except:\n",
    "            print(f\"error : {file_path}\")\n",
    "            return\n",
    "        \n",
    "        points = [[annotation_points[i], annotation_points[i+1]] for i in range(0, len(annotation_points), 2)]\n",
    "\n",
    "        file_name = file_path.split(\"\\\\\")[-1]\n",
    "        # root = \"/home/gon/dev_ws/deeplearning-repo-4/test/ai_hub_dataset/dataset/\"\n",
    "        label_file_path = \"./dataset/labels/\" + file_name\n",
    "        image_file_path = \"images/\" + file_name.split(\".\")[0] + \".jpg\"\n",
    "\n",
    "        labelme_json = {\n",
    "            \"version\": \"5.5.0\",\n",
    "            \"flags\": {},\n",
    "            \"shapes\": [\n",
    "                {\n",
    "                    \"label\": str(data[\"dataset\"][\"dataset.category\"]),\n",
    "                    \"points\": points,\n",
    "                    \"group_id\": None,\n",
    "                    \"description\": \"\",\n",
    "                    \"shape_type\": \"polygon\",\n",
    "                    \"flags\": {},\n",
    "                    \"mask\": None\n",
    "                }, \n",
    "            ],\n",
    "            \"imagePath\": image_file_path,\n",
    "            \"imageHeight\": data[\"dataset\"][\"dataset.height\"],\n",
    "            \"imageWidth\": data[\"dataset\"][\"dataset.width\"],\n",
    "        }\n",
    "\n",
    "        with open(label_file_path, \"w\") as json_file:\n",
    "            json.dump(labelme_json, json_file, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error : ./dataset/origin_labels\\01_sou_023744_118717_back_04bottom_01pants_woman.json\n",
      "error : ./dataset/origin_labels\\01_sou_033912_169560_wear_01outer_03jumper_woman.json\n",
      "error : ./dataset/origin_labels\\01_sou_043203_216013_wear_01outer_04cardigan_woman.json\n",
      "error : ./dataset/origin_labels\\01_sou_043434_217169_wear_01outer_04cardigan_woman.json\n",
      "error : ./dataset/origin_labels\\01_sou_043434_217170_wear_01outer_04cardigan_woman.json\n",
      "error : ./dataset/origin_labels\\01_sou_043437_217183_wear_01outer_04cardigan_woman.json\n",
      "error : ./dataset/origin_labels\\01_sou_070711_353552_back_02top_03sweater_man.json\n",
      "error : ./dataset/origin_labels\\01_sou_071484_357417_back_02top_04shirt_woman.json\n",
      "error : ./dataset/origin_labels\\01_sou_081174_405869_wear_01outer_04cardigan_woman.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file_path in origin_json_file_list:\n",
    "    convert_to_labelme(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  AI-HUB dataset 를 top - under 두 카테고리로만 정리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292627\n"
     ]
    }
   ],
   "source": [
    "origin_json_file_list = get_all_paths(\"./dataset/origin_labels\")\n",
    "print(len(origin_json_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_labelme(file_path):\n",
    "    with open(file_path, \"r\") as read_json:\n",
    "        data = json.load(read_json)\n",
    "\n",
    "        try:\n",
    "            annotation_points = data[\"annotation\"][0][\"annotation_point\"]\n",
    "        except:\n",
    "            print(f\"error : {file_path}\")\n",
    "            return\n",
    "        \n",
    "        points = [[annotation_points[i], annotation_points[i+1]] for i in range(0, len(annotation_points), 2)]\n",
    "\n",
    "        file_name = file_path.split(\"\\\\\")[-1]\n",
    "        # root = \"/home/gon/dev_ws/deeplearning-repo-4/test/ai_hub_dataset/dataset/\"\n",
    "        label_file_path = \"./dataset/labels/\" + file_name\n",
    "        image_file_path = \"images/\" + file_name.split(\".\")[0] + \".jpg\"\n",
    "\n",
    "        labelme_json = {\n",
    "            \"version\": \"5.5.0\",\n",
    "            \"flags\": {},\n",
    "            \"shapes\": [\n",
    "                {\n",
    "                    \"label\": data[\"dataset\"][\"dataset.category\"] >= 12? \"1\":\"0\",\n",
    "                    \"points\": points,\n",
    "                    \"group_id\": None,\n",
    "                    \"description\": \"\",\n",
    "                    \"shape_type\": \"polygon\",\n",
    "                    \"flags\": {},\n",
    "                    \"mask\": None\n",
    "                }, \n",
    "            ],\n",
    "            \"imagePath\": image_file_path,\n",
    "            \"imageHeight\": data[\"dataset\"][\"dataset.height\"],\n",
    "            \"imageWidth\": data[\"dataset\"][\"dataset.width\"],\n",
    "        }\n",
    "\n",
    "        with open(label_file_path, \"w\") as json_file:\n",
    "            json.dump(labelme_json, json_file, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_pjt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
